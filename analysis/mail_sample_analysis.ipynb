{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5b4a2d",
   "metadata": {},
   "source": [
    "# Mail Sample Analysis\n",
    "\n",
    "Analyzing email HTML body and cleaning it with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: ğŸ˜º\n",
      "Sender: The Neuron <theneuron@newsletter.theneurondaily.com>\n",
      "Date: 2025-12-23T19:37:18Z\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the JSON file\n",
    "data_path = Path('../test/mail_sample.json')\n",
    "with open(data_path, 'r') as f:\n",
    "    mail_data = json.load(f)\n",
    "\n",
    "print(f\"Subject: {mail_data['subject']}\")\n",
    "print(f\"Sender: {mail_data['sender']}\")\n",
    "print(f\"Date: {mail_data['date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3078f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original HTML body size: 103,995 characters\n",
      "Original HTML body size: 101.56 KB\n",
      "\n",
      "Cleaned text size: 10,908 characters\n",
      "Cleaned text size: 10.65 KB\n",
      "\n",
      "Size reduction: 89.5%\n",
      "Removed: 93,087 characters\n",
      "\n",
      "âœ“ Saved cleaned text to ../data/mail_sample_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "# Get the HTML body\n",
    "html_body = mail_data['body']\n",
    "\n",
    "# Check original size\n",
    "original_size = len(html_body)\n",
    "print(f\"Original HTML body size: {original_size:,} characters\")\n",
    "print(f\"Original HTML body size: {original_size / 1024:.2f} KB\")\n",
    "\n",
    "# Clean HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_body, 'html.parser')\n",
    "\n",
    "# Extract text content\n",
    "cleaned_text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "# Check cleaned size\n",
    "cleaned_size = len(cleaned_text)\n",
    "print(f\"\\nCleaned text size: {cleaned_size:,} characters\")\n",
    "print(f\"Cleaned text size: {cleaned_size / 1024:.2f} KB\")\n",
    "\n",
    "# Calculate reduction\n",
    "reduction_percent = ((original_size - cleaned_size) / original_size) * 100\n",
    "print(f\"\\nSize reduction: {reduction_percent:.1f}%\")\n",
    "print(f\"Removed: {original_size - cleaned_size:,} characters\")\n",
    "\n",
    "# Save cleaned body to file\n",
    "output_path = Path('../data/mail_sample_cleaned.txt')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(f\"\\nâœ“ Saved cleaned text to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9233df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of cleaned text (first 1000 characters):\n",
      "================================================================================\n",
      "ğŸ˜º YouTube lets you generate games now\n",
      "PLUS: New AI GLM 4.7, and OpenTinker for the nerds Â â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€ŒÂ â€Œ\n",
      "December 23, 2025 Â  |\n",
      "Listen Online\n",
      "|\n",
      "Read Online\n",
      "Welcome, humans.\n",
      "In case you were confused and didnâ€™t already think 2025 was the year of Google (\n",
      "Revenge of the Google? The Google Strikes Back? Google 2: Electric Boogaloo?),\n",
      "the company just dropped a\n",
      "year-end recap of their 60 biggest AI releases in 2025\n",
      ".\n",
      "Sixty!\n",
      "That's, like, â€œwell moreâ€ than one per\n",
      "week!\n",
      "Whatever you call it, this was truly Google's victory lap / revenge tour year.\n",
      "The highlights: Gemini 2.5 in March, Gemini 3 in November (their â€œne\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display a preview of the cleaned text\n",
    "print(\"Preview of cleaned text (first 1000 characters):\")\n",
    "print(\"=\" * 80)\n",
    "print(cleaned_text[:1000])\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-mail-parser-aggregator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
